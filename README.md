# Text-Generation
This project explores the capabilities of text generation using LSTM (Long Short-Term Memory) and GPT (Generative Pre-trained Transformer) models. It involves training and evaluating both approaches on a similar dataset to understand their performance, strengths, and weaknesses in generating coherent and contextually relevant text.


<img width="1919" height="929" alt="Screenshot 2025-07-22 130632" src="https://github.com/user-attachments/assets/10edf4a2-9b5e-4f42-b72c-683bf7a3d63a" />

<img width="1919" height="928" alt="Screenshot 2025-07-22 130525" src="https://github.com/user-attachments/assets/ffdc1415-769a-41f3-896d-7117db186359" />
<img width="1919" height="925" alt="Screenshot 2025-07-22 130550" src="https://github.com/user-attachments/assets/e2fede71-8cfb-4b8b-9c67-8e1c6959b71b" />
<img width="1919" height="927" alt="Screenshot 2025-07-22 130445" src="https://github.com/user-attachments/assets/a34ec674-7b67-4cfc-af21-84c90fe69ca1" />





